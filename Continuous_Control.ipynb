{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Unity ML-agent library has a lot of simulation evnrionments to test intelligent agent models. \n",
    "This notebook solves one of the envrionnment with a task name called **Reacher**\n",
    "The task is to move a 2-joint robot arm to a specific position. \n",
    "As the robot arm correctly follows the guided position, more points can be obtained.\n",
    "In this notebook, the agent uses a Deep Deterministic Policy Gradient model (DDPG) to learn an intelligent behavior to meet the objective and the environment will have 20 agents and robot arms.\n",
    "All the code is self contained in a notebook for a potential modification of any logic. \n",
    "Each component is separated to its own cell, so it can be easily written to a separate file using \"%writefile\" if necessary\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### 1. Environment\n",
    "#### Requirements\n",
    "- Linux based OS\n",
    "- Python (dependency is listed in requirements.txt)\n",
    "- [Linux 20-arm Reacher environment](https://s3-us-west-1.amazonaws.com/udacity-drlnd/P2/Reacher/Reacher_Linux.zip)\n",
    "\n",
    "The following code will import modules and packages required to train and evaluate a Actor/Critic models with DDPG algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_size -> 5.0\n",
      "\t\tgoal_speed -> 1.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "# Unzip the Reacher environment file and pass the path of \"Reacher.x86_64\" to UnityEnvironment as an argument\n",
    "env = UnityEnvironment(file_name='/data/Reacher_Linux_NoVis/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The environment spec\n",
    "- Number of agent: 20\n",
    "- State space: 33 dimensional vector (for each agent)\n",
    "- Action space: 4 numbers of range [-1, 1]\n",
    "- Reward: +0.1 (If the agent's arm is located at a guided location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 20\n",
      "Size of each action: 4\n",
      "There are 20 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [  0.00000000e+00  -4.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00  -1.00000000e+01   0.00000000e+00\n",
      "   1.00000000e+00  -0.00000000e+00  -0.00000000e+00  -4.37113883e-08\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   5.75471878e+00  -1.00000000e+00\n",
      "   5.55726624e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "  -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implementation\n",
    "\n",
    "#### ActorModel\n",
    "- **Description**\n",
    "    - Standard fully-connected NN with a Batchnorm layer and ReLU activation. This model is used to model the deterministic policy, so it outputs 4 continous action vectors.\n",
    "- **Architecture**\n",
    "    - 2 Hidden layers (128, 64 nodes)\n",
    "    - 1 Output layer (4 nodes)\n",
    "    - Each hidden layer uses ReLU and Batchnorm1d\n",
    "    - Final output layer is a single linear layer with tanh activation to conform to the action value range\n",
    "- **Discussion**\n",
    "    - The initial hidden layer expanded the number of nodes for exploring rich feature space\n",
    "    - ReLU and Batchnorm layer are used for stable training\n",
    "    - Final output layer is combined with tanh to generate action of a correct range\n",
    " \n",
    "#### CriticModel\n",
    "- **Description**\n",
    "    - A fully-connected NN which combines a state and action to generate Q value for a given state, action pair.\n",
    "- **Architecture**\n",
    "    - 2 Hidden layers (128, 128 nodes)\n",
    "    - 1 Output layer (1 node)\n",
    "    - Each hidden layer uses ReLU\n",
    "    - State value is normalized by BatchNorm1d\n",
    "    - Final output layer is a single linear layer without any activation function\n",
    "- **Discussion**\n",
    "    - To deal with different characteristics of state and action, they're embedded to the first hidden layer separately\n",
    "        - The first hidden layer 128 nodes are divided with the ratio of state and action dimensions (33 to 4)\n",
    "    - The initial hidden layer expanded the number of nodes for exploring rich feature space\n",
    "    - The number of nodes are decided with experiments\n",
    "    - Batchnorm layer is removed because it gives worse performance\n",
    "    - Final output layer directly generates an action value\n",
    " \n",
    "\n",
    "#### Buffer\n",
    "- **Description**\n",
    "    - Replay buffer used in a DDPG algorithm. It is a standard deque with a sampling function\n",
    "- **Interface**\n",
    "    - Initialization\n",
    "        - sz: Replay buffer size\n",
    "        - pf: Probability function for sampling\n",
    "    - put(obj): Stored obj in a replay buffer\n",
    "    - sample(n): Sample n objects from a replay buffer\n",
    "    - update_pf(pf): Update the probability function pf\n",
    "- **Discussion**\n",
    "    - pf can be used for prioritized experience replay\n",
    "    - It was used to explore the potential benefit of the skew in experience sampling, but was default to uniform sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorModel(nn.Module):\n",
    "    def __init__(self, nfs = [33, 128, 64, 4]):\n",
    "        super().__init__()\n",
    "        self.nfs = nfs\n",
    "        \n",
    "        dr = nn.Dropout()\n",
    "        layers = [nn.BatchNorm1d(self.nfs[0])]\n",
    "        for nif, nof in zip(self.nfs[:-2], self.nfs[1:-1]):\n",
    "            layers.append(nn.Linear(nif, nof))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(nof))\n",
    "#             layers.append(dr)\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.olin = nn.Linear(self.nfs[-2], self.nfs[-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        f = self.layers(x)\n",
    "        actions = F.tanh(self.olin(f))\n",
    "        \n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticModel(nn.Module):\n",
    "    def __init__(self, nfs = [128, 128, 1], ns=33, na=4):\n",
    "        super().__init__()\n",
    "        self.nfs = nfs\n",
    "        self.ns = ns\n",
    "        self.na = na\n",
    "        \n",
    "        dr = nn.Dropout()\n",
    "        \n",
    "        num_state_fs = int(self.nfs[0] * self.ns /(self.ns+self.na))\n",
    "        num_act_fs = self.nfs[0] - num_state_fs\n",
    "        \n",
    "        self.state_fs = nn.Sequential(\n",
    "            nn.BatchNorm1d(self.ns), \n",
    "            nn.Linear(self.ns, num_state_fs),\n",
    "            nn.ReLU()#,\n",
    "#             nn.BatchNorm1d(num_state_fs)\n",
    "        )\n",
    "        self.act_fs = nn.Sequential(\n",
    "            nn.Linear(self.na, num_act_fs),\n",
    "            nn.ReLU()#,\n",
    "#             nn.BatchNorm1d(num_act_fs)\n",
    "        )\n",
    "        \n",
    "        layers = []\n",
    "        for nif, nof in zip(self.nfs[:-2], self.nfs[1:-1]):\n",
    "            layers.append(nn.Linear(nif, nof))\n",
    "            layers.append(nn.ReLU())\n",
    "#             layers.append(nn.BatchNorm1d(nof))\n",
    "#             layers.append(dr)\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.olin = nn.Linear(self.nfs[-2], self.nfs[-1])\n",
    "    \n",
    "    def forward(self, x, a):\n",
    "        xf = self.state_fs(x)\n",
    "        af = self.act_fs(a)\n",
    "        f = self.layers(torch.cat((xf, af), 1))\n",
    "        values = self.olin(f)\n",
    "        \n",
    "        return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, sz=50000, pf=70):# pf=lambda x: 1):\n",
    "        self.buf = deque(maxlen=sz)\n",
    "        self.px = deque(maxlen=sz)\n",
    "        self.temp = []\n",
    "        self.pf = pf\n",
    "        \n",
    "    def update_pf(self, pf=lambda x: 1):\n",
    "        self.pf = pf\n",
    "        \n",
    "    def put(self, sars):\n",
    "        self.temp.append(sars)\n",
    "        if sars[2] != 0:\n",
    "            self.buf.extend(self.temp)\n",
    "            self.px.extend([self.pf(x) for x in range(-len(self.temp) + 1, 1)])\n",
    "#             newbuf = self.temp[np.random.randint(self.pf//70):self.pf:int(self.pf//70)]\n",
    "#             self.buf.extend(newbuf)\n",
    "#             self.px.extend([1 for _ in range(len(newbuf))])\n",
    "            self.temp.clear()\n",
    "    \n",
    "    def sample(self, n=64):\n",
    "        bufsz = len(self.buf)\n",
    "        assert bufsz == len(self.px)\n",
    "        n = bufsz if bufsz <= n else n\n",
    "        idxs = np.random.choice(range(bufsz), n, replace=False, p=(self.px / np.sum(self.px)))\n",
    "        samples = [self.buf[i] for i in idxs]\n",
    "        \n",
    "        states, actions, rewards, nstates = zip(*samples)\n",
    "        return states, actions, rewards, nstates\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "    \n",
    "    def clear(self):\n",
    "        self.temp = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "- **Description**\n",
    "    - Standard DDPG training algorithm\n",
    "- **Detail**\n",
    "    1. Initialize the models(2 actors, 2 critics) and hyperparameters\n",
    "    2. The current-actor-model (agent) interacts with the Reaper Unity environment to collect tuples (state, action, reward, next-state)\n",
    "        2-a. Exploration in continuous space is implemented using a Gaussian noise. The noise's standard deviation is controlled by an epsilon parameter\n",
    "        2-b. Due to the probabilistic nature of the noise, the action is clipped to range [-1, 1]\n",
    "    3. The collected tuples (experiences) are stored in a replay buffer\n",
    "    4. The experience is randomly batch-sampled every 20 steps(\"train_step\") and used to train the NN model\n",
    "        4-a. Due to the multi-agent nature of the environment, The single batch size is 512 and 10 successive gradient updates happen in a single training procedure.\n",
    "        4-b. The training procedure follows a standard DDPG algorithm\n",
    "            4-b1. Current-critic-model is trained with TD-update using Target-actor/critic-models\n",
    "            4-b2. Current-actor model is trained with Q value using Target-critic-model\n",
    "            4-b3. Target-actor/critic-models are soft-updated with current models using a small mixture ratio(tau)\n",
    "    5. 2-4 are repeated until the average score(return) reaches the objective score (13)\n",
    "- **Hyperparameters**\n",
    "\n",
    "|name|value|description|\n",
    "|--|--|--|\n",
    "|n_episodes|500|Max number of episodes to explore|\n",
    "|max_t|3000|Max number of time steps of each episode|\n",
    "|bn|512|Training single batch size|\n",
    "|gamma|0.99|Return discounting factor|\n",
    "|eps, eps_decay|0.25, 0.99|Parameter to control noise standard deviation (for exploration) |\n",
    "|tau, tau_decay, tau_min|0.001, 0.9999, 0.001|Target network soft-update parameters|\n",
    "|train_step|20|Frequency of the training|\n",
    "|train_cnt|10|Number of gradient-updates in a single training procedure|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 500\n",
    "max_t = 3000\n",
    "bn = 512\n",
    "\n",
    "gamma = 0.99\n",
    "eps = 0.25\n",
    "eps_decay = 0.99; #eps_min=0.05\n",
    "tau = 0.001\n",
    "tau_decay = 0.9999; tau_min=0.001\n",
    "train_step = 20; train_cnt = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode no: 1, Average score: 0.607999986410141\n",
      "Episode no: 3, Average score: 0.5481666544141869\n",
      "Episode no: 5, Average score: 0.5798999870382249\n",
      "Episode no: 7, Average score: 0.6012142722760992\n",
      "Episode no: 9, Average score: 0.6408333190095922\n",
      "Episode no: 11, Average score: 0.6475908946161243\n",
      "Episode no: 13, Average score: 0.6586922929693872\n",
      "Episode no: 15, Average score: 0.6781333181758722\n",
      "Episode no: 17, Average score: 0.71102939587196\n",
      "Episode no: 19, Average score: 0.747552614869844\n",
      "Episode no: 21, Average score: 0.8598095045912835\n",
      "Episode no: 23, Average score: 0.9650434566904671\n",
      "Episode no: 25, Average score: 1.070639976069331\n",
      "Episode no: 27, Average score: 1.2330370094765115\n",
      "Episode no: 29, Average score: 1.4217068647740987\n",
      "Episode no: 31, Average score: 1.6057741576565367\n",
      "Episode no: 33, Average score: 1.8743635944683443\n",
      "Episode no: 35, Average score: 2.3517142331493748\n",
      "Episode no: 37, Average score: 2.877391827577171\n",
      "Episode no: 39, Average score: 3.3238845410900044\n",
      "Episode no: 41, Average score: 3.6982804051418676\n",
      "Episode no: 43, Average score: 4.084081304062504\n",
      "Episode no: 45, Average score: 4.492888788464996\n",
      "Episode no: 47, Average score: 4.74837223429114\n",
      "Episode no: 49, Average score: 5.0550713155817775\n",
      "Episode no: 51, Average score: 5.305205763772348\n",
      "Episode no: 53, Average score: 5.462764028841102\n",
      "Episode no: 55, Average score: 5.718654417632655\n",
      "Episode no: 57, Average score: 5.99475425197171\n",
      "Episode no: 59, Average score: 6.439440534033386\n",
      "Episode no: 61, Average score: 6.962893286990154\n",
      "Episode no: 63, Average score: 7.614388718694034\n",
      "Episode no: 65, Average score: 8.290738276226016\n",
      "Episode no: 67, Average score: 8.988275918499356\n",
      "Episode no: 69, Average score: 9.677021522832142\n",
      "Episode no: 71, Average score: 10.349816670072014\n",
      "Episode no: 73, Average score: 10.781760032981481\n",
      "Episode no: 75, Average score: 11.206759749509393\n",
      "Episode no: 77, Average score: 11.63648675289126\n",
      "Episode no: 79, Average score: 12.039297199255161\n",
      "Episode no: 81, Average score: 12.404962685690434\n",
      "Episode no: 83, Average score: 12.788517786443501\n",
      "Episode no: 85, Average score: 13.318164408198173\n",
      "Episode no: 87, Average score: 13.88816635624195\n",
      "Episode no: 89, Average score: 14.4575165307819\n",
      "Episode no: 91, Average score: 14.980191972858917\n",
      "Episode no: 93, Average score: 15.451671697638968\n",
      "Episode no: 95, Average score: 15.899710170929563\n",
      "Episode no: 97, Average score: 16.307948089097195\n",
      "Episode no: 99, Average score: 16.749999625608325\n",
      "Episode no: 101, Average score: 17.314529612990096\n",
      "Episode no: 103, Average score: 18.051984596506692\n",
      "Episode no: 105, Average score: 18.730449581341816\n",
      "Episode no: 107, Average score: 19.390659566584976\n",
      "Episode no: 109, Average score: 20.070269551394507\n",
      "Episode no: 111, Average score: 20.762889535913242\n",
      "Episode no: 113, Average score: 21.45722452039365\n",
      "Episode no: 115, Average score: 22.15778450473491\n",
      "Episode no: 117, Average score: 22.886494488446974\n",
      "Episode no: 119, Average score: 23.627404471886344\n",
      "Episode no: 121, Average score: 24.243164458123037\n",
      "Episode no: 123, Average score: 24.89277444360313\n",
      "Episode no: 125, Average score: 25.58551942811907\n",
      "Episode no: 127, Average score: 26.20693441422936\n",
      "Episode no: 129, Average score: 26.8338144002175\n",
      "Episode no: 131, Average score: 27.465789386091757\n",
      "Episode no: 133, Average score: 28.07371937250346\n",
      "Episode no: 135, Average score: 28.63573435994145\n",
      "Episode no: 137, Average score: 29.165999348089098\n",
      "Episode no: 139, Average score: 29.69757433620747\n",
      "Episode no: 141, Average score: 30.23942432409618\n",
      "Models are saved and training is finished\n",
      "Total score (averaged over agents) this episode: 37.65449915835634\n",
      "CPU times: user 52min 30s, sys: 4min 24s, total: 56min 54s\n",
      "Wall time: 1h 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "actor_current = ActorModel().to(device)\n",
    "actor_target = ActorModel().to(device)\n",
    "critic_current = CriticModel().to(device)\n",
    "critic_target = CriticModel().to(device)\n",
    "\n",
    "# actor_target.eval()\n",
    "# critic_target.eval()\n",
    "\n",
    "replay_buf = Buffer(pf=lambda x: 1)\n",
    "\n",
    "total_scores = []\n",
    "actor_opt = optim.Adam(actor_current.parameters())\n",
    "critic_opt = optim.Adam(critic_current.parameters())\n",
    "\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    # Score check and NN weight save\n",
    "    if i % 2 == 1:\n",
    "        avg_score = np.mean(total_scores[-100:])\n",
    "        print(f\"Episode no: {i}, Average score: {avg_score}\")\n",
    "\n",
    "        if avg_score >= 30 and np.mean(total_scores[-10:]) >= 30:\n",
    "            torch.save(actor_current.state_dict(), \"a_current.pth\")\n",
    "            torch.save(actor_target.state_dict(), \"a_target.pth\")\n",
    "            torch.save(critic_current.state_dict(), \"c_current.pth\")\n",
    "            torch.save(critic_target.state_dict(), \"c_target.pth\")\n",
    "            print(\"Models are saved and training is finished\")\n",
    "            break\n",
    "        \n",
    "    env_info = env.reset(train_mode=True)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "    for t in range(max_t):\n",
    "        sts = torch.tensor(states, dtype=torch.float, device=device)\n",
    "        \n",
    "        #####################################################\n",
    "        # Interact with environment (eps: exploration ratio)\n",
    "        #####################################################\n",
    "        actor_current.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = actor_current(sts)\n",
    "            actions += (eps * torch.randn(actions.shape, device=device))\n",
    "            actions = torch.clamp(actions, -1, 1).detach().cpu().numpy()\n",
    "        actor_current.train()\n",
    "        \n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        \n",
    "        \n",
    "        for s, a, r, ns in zip(states, actions, rewards, next_states):\n",
    "            replay_buf.put((s, a, r, ns))\n",
    "            \n",
    "            \n",
    "        ###########################################################\n",
    "        # Training (Actor-loss: Average Q-value, Critic-loss: MSE)\n",
    "        ########################################################### \n",
    "        if t % train_step == 0 and len(replay_buf) >= 1024:\n",
    "            for k in range(train_cnt):\n",
    "                ss, aa, rr, nss = [torch.tensor(arr, dtype=torch.float, device=device).view(len(arr), -1) \n",
    "                                   for arr in replay_buf.sample(bn)]\n",
    "                \n",
    "                # Critic Current Model\n",
    "                na = actor_target(nss)\n",
    "                nq = critic_target(nss, na)\n",
    "\n",
    "                curq = critic_current(ss, aa)\n",
    "                tgtq = rr + gamma * nq\n",
    "\n",
    "                loss = F.mse_loss(curq, tgtq)\n",
    "                critic_opt.zero_grad()\n",
    "                loss.backward()\n",
    "                critic_opt.step()\n",
    "\n",
    "                # Actor Current Model\n",
    "                na = actor_current(ss)\n",
    "                nq = critic_target(ss, na)\n",
    "#                 nq = critic_current(ss, na)\n",
    "                loss = -nq.mean()\n",
    "                actor_opt.zero_grad()\n",
    "                loss.backward()\n",
    "                actor_opt.step()\n",
    "\n",
    "                # Target Model\n",
    "                mixr = np.max([tau, tau_min])\n",
    "                for tparam, cparam in zip(actor_target.parameters(), actor_current.parameters()):\n",
    "                    tparam.data.copy_(mixr * cparam.data + (1-mixr) * tparam.data)\n",
    "                for tparam, cparam in zip(critic_target.parameters(), critic_current.parameters()):\n",
    "                    tparam.data.copy_(mixr * cparam.data + (1-mixr) * tparam.data)\n",
    "                tau *= tau_decay\n",
    "        \n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "                        \n",
    "    eps *= eps_decay\n",
    "    total_scores.append(scores.mean())\n",
    "                        \n",
    "    \n",
    "                        \n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following chart show the average score of 20 agents in each episode as it trains. <br>\n",
    "A few interesting observations are\n",
    "- There is a sequence of episodes when the average score is quite static. Its interpretaion in a real world could be the required time to learn new skills.\n",
    "- After the static time, there is a significant increase in the average score, this can be another proof that a meaningful skill was learned and utilized to solve the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Average score')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fde44592320>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(total_scores)\n",
    "plt.title(\"Average score of 20 agents in each episode\")\n",
    "plt.xlabel(\"# episode\")\n",
    "plt.ylabel(\"Average score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agent Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 37.39699916411191\n"
     ]
    }
   ],
   "source": [
    "actor_current.load_state_dict(torch.load('actor_current.pth'))\n",
    "actor_network = actor_current\n",
    "\n",
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "for t in range(max_t):\n",
    "    sts = torch.tensor(states, dtype=torch.float, device=device)\n",
    "\n",
    "    actor_network.eval()\n",
    "    with torch.no_grad():\n",
    "        actions = actor_network(sts).detach().cpu().numpy()\n",
    "    actor_network.train()\n",
    "    \n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "        \n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Batchnorm layer is usually known to help the NN optimization, but it seemed to worsen the performnace with the given NN-architecture and task/environment. A few hypotheses can be explored with a detailed look at the training steps\n",
    "    - The reward is quite big in this environment. With a simple NN-architecture heavily using BN, one signle output layer needs to deal with the big reward scale, which might not be appropriate\n",
    "    - BN might not help much in RL setting where the input data has a very big variance. (considering the usual batch size of 32 ~ 256 time-steps)\n",
    "    - NN is too small and simple. BN might not have a big impact in a small NN\n",
    "\n",
    "\n",
    "2. In DDPG algorithm, the actor can be trained with a Q-value estimate of critic current or target network. It can be expected that the current encourages faster training and the target can help more stable training. This expectation can be tested with an experiment.\n",
    "\n",
    "\n",
    "3. Other policy-gradient methods can be tried and compared against DDPG algorithm "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
